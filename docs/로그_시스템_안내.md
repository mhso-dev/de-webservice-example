# 웹 서비스 로그 시스템 안내

이 문서는 웹 서비스의 로그 시스템에 대한 상세 설명을 제공합니다. 로그는 시스템의 동작 상태를 모니터링하고, 사용자 행동을 분석하는 데 중요한 데이터 소스입니다.

## 1. 로그 개요

본 웹 서비스는 크게 두 가지 유형의 로그를 생성합니다:

1. **시스템 로그**: 애플리케이션의 동작, 오류, 성능 관련 정보를 기록
2. **사용자 활동 로그**: 사용자의 행동과 상호작용을 기록

모든 로그는 JSON 형식으로 저장되어 데이터 분석과 처리가 용이하도록 설계되어 있습니다.

## 2. 로그 저장 위치

로그 파일은 다음 위치에 저장됩니다:

- **시스템 로그**: `01_webservice/logs/app.log`
- **사용자 활동 로그**: `01_webservice/logs/user_activity.log`
- **데이터베이스 로그**: 모든 사용자 활동은 `UserActivityLog` 테이블에도 저장됩니다.

## 3. 로그 구조

### 3.1 시스템 로그 구조

시스템 로그는 애플리케이션의 상태와 동작을 모니터링하기 위한 로그입니다. 주요 필드는 다음과 같습니다:

```json
{
  "timestamp": "2025-03-12T16:02:01.888",
  "level": "INFO",
  "message": "애플리케이션 시작",
  "module": "__init__"
}
```

| 필드 | 설명 |
|------|------|
| timestamp | 로그가 생성된 시각 (ISO 8601 형식) |
| level | 로그 수준 (INFO, WARNING, ERROR, DEBUG 등) |
| message | 로그 메시지 |
| module | 로그를 생성한 모듈 이름 |

### 3.2 사용자 활동 로그 구조

사용자 활동 로그는 사용자의 행동과 상호작용을 기록합니다. 이 로그는 다양한 이벤트 유형에 따라 필드가 달라질 수 있지만, 공통적인 필드는 다음과 같습니다:

```json
{
  "timestamp": "2025-03-12T16:20:54.696225",
  "session_id": ".eJxNyjsOwjAMANC7eKbUztfkHOxRIE4bqbQVCRPi7pSN-b03xPKUNkMoaWlygiW1Hvc0CQQY06vP47JNdYU_ib0-fqxQ2QH1QOpKLigMFs-GrdHu2E1aq9saaz7m5WYKF2_xbom1F6OKZsxC7NlRJvh8AXOoKBg.Z9E10g.jvFBbO_v-uLH6oRTgj0dMUoWPsQ",
  "event_type": "search",
  "user_id": null,
  "entity_type": null,
  "entity_id": null,
  "endpoint": "main.search",
  "method": "GET",
  "path": "/search",
  "args": {"q": "스마트폰"},
  "form": {},
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "referrer": null,
  "response_status": 200,
  "process_time": 0.093471
}
```

| 필드 | 설명 |
|------|------|
| timestamp | 로그가 생성된 시각 (ISO 8601 형식) |
| session_id | 사용자 세션 식별자 |
| event_type | 이벤트 유형 (login, view, search 등) |
| user_id | 사용자 ID (로그인한 경우) |
| entity_type | 상호작용한 엔티티 유형 (product, category 등) |
| entity_id | 상호작용한 엔티티 ID |
| endpoint | 요청이 처리된 Flask 엔드포인트 |
| method | HTTP 메서드 (GET, POST 등) |
| path | 요청 URL 경로 |
| args | URL 쿼리 매개변수 |
| form | 폼 데이터 |
| ip_address | 사용자 IP 주소 |
| user_agent | 사용자 브라우저/클라이언트 정보 |
| referrer | 이전 페이지 URL |
| response_status | HTTP 응답 상태 코드 |
| process_time | 요청 처리 시간 (초) |

## 4. 주요 이벤트 유형

사용자 활동 로그에서 기록되는 주요 이벤트 유형과 각 이벤트에 특화된 필드를 설명합니다.

### 4.1 로그인 관련 이벤트

#### 4.1.1 로그인 시도 (login_attempt)

```json
{
  "timestamp": "2025-03-12T16:22:29.513128",
  "session_id": "no_session",
  "event_type": "login_attempt",
  "user_id": null,
  "endpoint": "auth.login",
  "method": "POST",
  "path": "/auth/login",
  "form": {"email": "admin@example.com", "password": "******"},
  "username_attempt": "admin@example.com",
  "response_status": 200
}
```

#### 4.1.2 로그인 실패 (login_failed)

```json
{
  "timestamp": "2025-03-12T16:22:21.631083",
  "event_type": "login_failed",
  "username_attempt": "admin",
  "session_id": "950e8de422364f5b377d1492cfec5a1e",
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "reason": "invalid_credentials"
}
```

#### 4.1.3 로그인 성공 (login_success)

로그인 성공 시에는 `user_id`가 설정됩니다.

```json
{
  "timestamp": "2025-03-12T16:20:50.509",
  "session_id": ".eJxNyksOwiAQANC7zNrameHTyDnck6FAS4KtKbgy3t26c_3eG3w-UlvBZaktXaBK6_4pSwIHo7z6OtZ9KRv8ie_l8WNGNgOqgfhO1jE7vl0NKUJz7pZaK_vmSzwnBppJa2utiZMKOSCimKCintjOkuDzBXDhKHc",
  "event_type": "login_success",
  "user_id": 1,
  "endpoint": "auth.login",
  "method": "POST",
  "path": "/auth/login",
  "ip_address": "127.0.0.1",
  "user_agent": "Mozilla/5.0",
  "response_status": 200
}
```

### 4.2 페이지 조회 이벤트

#### 4.2.1 페이지 보기 (page_view)

```json
{
  "timestamp": "2025-03-12T16:22:30.552891",
  "session_id": ".eJxNyksOwiAQANC7zNrameHTyDnck6FAS4KtKbgy3t26c_3eG3w-UlvBZaktXaBK6_4pSwIHo7z6OtZ9KRv8ie_l8WNGNgOqgfhO1jE7vl0NKUJz7pZaK_vmSzwnBppJa2utiZMKOSCimKCintjOkuDzBXDhKHc.Z9E2NQ.yP2cjDa16Ii7FydlthN4uJq9kgo",
  "event_type": "page_view",
  "user_id": null,
  "entity_type": null,
  "entity_id": null,
  "endpoint": null,
  "method": "GET",
  "path": "/products",
  "args": {},
  "form": {},
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "referrer": null,
  "response_status": 308,
  "process_time": 0.000457
}
```

#### 4.2.2 상품 조회 (view)

```json
{
  "timestamp": "2025-03-12T16:22:42.536532",
  "session_id": "no_session",
  "event_type": "view",
  "user_id": null,
  "entity_type": "product",
  "entity_id": 1,
  "endpoint": "products.detail",
  "method": "GET",
  "path": "/products/1",
  "args": {},
  "form": {},
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "referrer": null,
  "response_status": 200,
  "process_time": 0.043177
}
```

### 4.3 검색 이벤트 (search)

```json
{
  "timestamp": "2025-03-12T16:20:54.696225",
  "session_id": ".eJxNyjsOwjAMANC7eKbUztfkHOxRIE4bqbQVCRPi7pSN-b03xPKUNkMoaWlygiW1Hvc0CQQY06vP47JNdYU_ib0-fqxQ2QH1QOpKLigMFs-GrdHu2E1aq9saaz7m5WYKF2_xbom1F6OKZsxC7NlRJvh8AXOoKBg.Z9E10g.jvFBbO_v-uLH6oRTgj0dMUoWPsQ",
  "event_type": "search",
  "user_id": null,
  "entity_type": null,
  "entity_id": null,
  "endpoint": "main.search",
  "method": "GET",
  "path": "/search",
  "args": {"q": "스마트폰"},
  "form": {},
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "referrer": null,
  "search_query": "스마트폰",
  "results_count": 0,
  "response_status": 200,
  "process_time": 0.093471
}
```

### 4.4 체류 시간 이벤트 (server_dwell_time)

```json
{
  "timestamp": "2025-03-12T16:22:30.552820",
  "event_type": "server_dwell_time",
  "user_id": null,
  "session_id": "0b1c1446665d73bfb000a5b3d4726cae",
  "product_id": null,
  "previous_path": "/auth/login",
  "current_path": "/products",
  "dwell_time_seconds": 1.039715,
  "user_agent": "Unknown",
  "ip_address": "127.0.0.1"
}
```

### 4.5 장바구니 관련 이벤트

```json
{
  "timestamp": "2025-03-12T16:21:20.116800",
  "session_id": ".eJxNyksOwiAQANC7zNrameHTyDnck6FAS4KtKbgy3t26c_3eG3w-UlvBZaktXaBK6_4pSwIHo7z6OtZ9KRv8ie_l8WNGNgOqgfhO1jE7vl0NKUJz7pZaK_vmSzwnBppJa2utiZMKOSCimKCintjOkuDzBXDhKHc.Z9E2NQ.yP2cjDa16Ii7FydlthN4uJq9kgo",
  "event_type": "cart_add",
  "user_id": 1,
  "entity_type": "product",
  "entity_id": 1,
  "endpoint": "cart.add",
  "method": "POST",
  "path": "/cart/add",
  "form": {"product_id": "1", "quantity": "2"},
  "ip_address": "127.0.0.1",
  "user_agent": "Unknown",
  "referrer": "/products/1",
  "response_status": 302,
  "process_time": 0.021256
}
```

## 5. 로그 생성 메커니즘

웹 서비스의 로그는 다음과 같은 방식으로 생성됩니다:

1. **자동 로깅**: 모든 HTTP 요청은 `@app.before_request`와 `@app.after_request` 훅을 통해 자동으로 로깅됩니다.

2. **명시적 로깅**: 중요한 사용자 활동(로그인, 로그아웃, 검색 등)은 해당 기능을 처리하는 코드에서 명시적으로 로깅됩니다.

3. **에러 로깅**: 애플리케이션에서 발생한 예외와 오류는 자동으로 로깅됩니다.

## 6. 로그 활용 방법

### 6.1 로그 분석 및 모니터링

로그 데이터는 다음과 같은 목적으로 활용할 수 있습니다:

1. **사용자 행동 분석**:
   - 어떤 페이지가 가장 인기 있는지
   - 검색 패턴 및 사용자 관심사
   - 사용자의 구매 경로 분석

2. **성능 모니터링**:
   - 응답 시간이 긴 엔드포인트 식별
   - 시스템 부하 패턴 분석

3. **보안 모니터링**:
   - 로그인 실패 패턴 감지
   - 비정상적인 접근 시도 탐지

### 6.2 로그 분석을 위한 도구 예시

로그 데이터는 다음과 같은 도구를 사용하여 분석할 수 있습니다:

1. **Python 스크립트**: pandas, matplotlib 등을 사용한 데이터 분석
2. **ELK 스택**: Elasticsearch, Logstash, Kibana를 사용한 로그 수집 및 시각화
3. **데이터 웨어하우스**: Snowflake, BigQuery 등을 사용한 대용량 로그 분석

## 7. 고급 로그 처리 및 분석 시스템

### 7.1 Fluentd를 활용한 로그 수집 및 전달

[Fluentd](https://www.fluentd.org/)는 로그 데이터를 효율적으로 수집, 변환, 전달하기 위한 오픈소스 데이터 수집기입니다. 웹 서비스의 로그 시스템과 Fluentd를 통합하면 다음과 같은 이점이 있습니다:

#### 7.1.1 Fluentd 설정 예시

```xml
<source>
  @type tail
  path /Users/mhso/working/data-eng-project/01_webservice/logs/user_activity.log
  pos_file /var/log/td-agent/user_activity.log.pos
  tag webservice.user_activity
  <parse>
    @type json
  </parse>
</source>

<filter webservice.user_activity>
  @type record_transformer
  <record>
    environment production
    service_name webservice
  </record>
</filter>

<match webservice.user_activity>
  @type elasticsearch
  host elasticsearch.example.com
  port 9200
  logstash_format true
  logstash_prefix webservice
  include_tag_key true
  tag_key @log_name
  flush_interval 5s
</match>
```

#### 7.1.2 Fluentd의 주요 기능

- **통합 로깅**: 여러 서버와 애플리케이션의 로그를 중앙 집중식으로 수집
- **실시간 처리**: 로그 데이터를 실시간으로 수집하고 전달
- **데이터 변환**: 로그 형식을 표준화하거나 필요한 필드를 추가/제거
- **다양한 출력 지원**: Elasticsearch, Amazon S3, MongoDB, Hadoop 등 다양한 저장소로 데이터 전달
- **신뢰성**: 버퍼링 메커니즘을 통해 데이터 손실 방지

#### 7.1.3 웹 서비스에서의 활용 사례

- **여러 서버의 로그 통합**: 웹 서버가 여러 대로 스케일 아웃된 경우 모든 서버의 로그를 중앙 집중식으로 관리
- **로그 데이터 강화**: IP 주소로부터 지리적 위치 정보 추가, 사용자 에이전트 파싱 등
- **이상 탐지를 위한 실시간 로그 분석**: 로그인 실패, 오류 증가 등을 모니터링

### 7.2 Apache Spark를 활용한 대용량 로그 처리

[Apache Spark](https://spark.apache.org/)는 대용량 데이터 처리를 위한 고속 분산 컴퓨팅 시스템입니다. 웹 서비스의 로그 데이터가 증가함에 따라 Spark를 활용하여 효율적으로 처리할 수 있습니다.

#### 7.2.1 Spark로 로그 처리하기 - 코드 예시

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, explode, window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType

# Spark 세션 생성
spark = SparkSession.builder \
    .appName("WebServiceLogAnalysis") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()

# 로그 스키마 정의
log_schema = StructType([
    StructField("timestamp", TimestampType(), True),
    StructField("session_id", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("user_id", IntegerType(), True),
    StructField("entity_type", StringType(), True),
    StructField("entity_id", IntegerType(), True),
    StructField("path", StringType(), True),
    StructField("ip_address", StringType(), True),
    StructField("response_status", IntegerType(), True),
    StructField("process_time", StringType(), True)
])

# JSON 로그 파일 읽기
logs_df = spark.read.json("hdfs:///logs/user_activity_*.log", schema=log_schema)

# 이벤트 유형별 집계
event_counts = logs_df.groupBy("event_type").count().orderBy("count", ascending=False)
event_counts.show()

# 제품별 조회 수 집계
product_views = logs_df.filter(
    (logs_df.event_type == "view") & (logs_df.entity_type == "product")
).groupBy("entity_id").count().orderBy("count", ascending=False)
product_views.show(10)  # 상위 10개 제품 조회

# 시간대별 사용자 활동 분석
hourly_activity = logs_df.groupBy(
    logs_df.timestamp.cast("date").alias("date"),
    hour(logs_df.timestamp).alias("hour")
).count().orderBy("date", "hour")
hourly_activity.show(24)  # 24시간 활동 보기

# 결과를 CSV 파일로 저장
product_views.write.csv("hdfs:///analytics/product_views", header=True)
hourly_activity.write.csv("hdfs:///analytics/hourly_activity", header=True)
```

#### 7.2.2 Spark의 주요 장점

- **대용량 처리**: 테라바이트 규모의 로그 데이터도 효율적으로 처리
- **인메모리 컴퓨팅**: 데이터를 메모리에 캐싱하여 반복 작업 성능 향상
- **고급 분석**: SQL, 머신러닝, 그래프 처리 등 다양한 분석 기능 제공
- **통합 플랫폼**: 배치 처리와 스트리밍 처리를 동일한 코드베이스로 처리 가능

#### 7.2.3 웹 서비스에서의 활용 사례

- **사용자 활동 패턴 분석**: 시간대별, 요일별 사용자 활동 패턴 파악
- **제품 인기도 분석**: 카테고리별, 기간별 제품 조회 및 구매 패턴 분석
- **이탈률 분석**: 사용자 세션 데이터를 분석하여 이탈 지점 파악
- **A/B 테스트 결과 분석**: 다양한 실험 그룹별 성과 비교

### 7.3 Spark Streaming을 활용한 실시간 로그 분석

[Spark Streaming](https://spark.apache.org/streaming/)은 Apache Spark의 확장 기능으로, 실시간 데이터 스트림을 처리할 수 있습니다. 웹 서비스의 로그를 실시간으로 분석하여 즉각적인 인사이트를 얻을 수 있습니다.

#### 7.3.1 Spark Streaming 구현 예시

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType

# Spark 스트리밍 세션 생성
spark = SparkSession.builder \
    .appName("WebServiceLogStreamingAnalysis") \
    .config("spark.sql.streaming.checkpointLocation", "/checkpoint") \
    .getOrCreate()

# 로그 스키마 정의
log_schema = StructType([
    StructField("timestamp", TimestampType(), True),
    StructField("session_id", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("user_id", IntegerType(), True),
    StructField("path", StringType(), True),
    StructField("response_status", IntegerType(), True)
])

# Kafka 스트림 구독
logs_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "webservice_logs") \
    .load()

# JSON 메시지 파싱
parsed_logs = logs_stream.select(
    from_json(col("value").cast("string"), log_schema).alias("log")
).select("log.*")

# 5분 윈도우 내 HTTP 오류 집계
error_counts = parsed_logs \
    .filter(col("response_status") >= 400) \
    .withWatermark("timestamp", "1 minute") \
    .groupBy(
        window(col("timestamp"), "5 minutes", "1 minute"),
        col("response_status")
    ) \
    .count()

# 실시간 로그인 시도 모니터링
login_attempts = parsed_logs \
    .filter(col("event_type").isin(["login_attempt", "login_success", "login_failed"])) \
    .withWatermark("timestamp", "1 minute") \
    .groupBy(
        window(col("timestamp"), "1 minute"),
        col("event_type"),
        col("ip_address")
    ) \
    .count()

# 스트리밍 쿼리 시작 - 콘솔에 출력
query1 = error_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

# 스트리밍 쿼리 시작 - 알림 시스템으로 출력
query2 = login_attempts.writeStream \
    .outputMode("update") \
    .foreachBatch(send_security_alerts) \
    .start()

# 스트리밍 대시보드 업데이트
query3 = parsed_logs.groupBy(
    window(col("timestamp"), "30 seconds"),
    col("path")
).count().writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("recent_traffic") \
    .start()

# 쿼리가 종료될 때까지 대기
query1.awaitTermination()
query2.awaitTermination()
query3.awaitTermination()

# 보안 알림 전송 함수
def send_security_alerts(df, batch_id):
    # 연속된 로그인 실패 탐지
    failed_logins = df.filter(col("event_type") == "login_failed") \
        .filter(col("count") >= 5)  # 5회 이상 실패
    
    for row in failed_logins.collect():
        # 보안 알림 전송 (이메일, Slack 등)
        send_alert(f"보안 경고: IP {row.ip_address}에서 {row.count}회 로그인 실패")
```

#### 7.3.2 Spark Streaming의 주요 기능

- **마이크로 배치 처리**: 데이터 스트림을 작은 배치로 처리하여 효율성과 내결함성 확보
- **통합 프로그래밍 모델**: 배치 처리와 동일한 API를 사용하여 스트리밍 처리 구현
- **복잡한 처리 지원**: 윈도우 연산, 조인, 집계 등 복잡한 처리 지원
- **다양한 데이터 소스 지원**: Kafka, Flume, Kinesis, TCP 소켓 등 다양한 소스에서 데이터 스트리밍

#### 7.3.3 웹 서비스에서의 활용 사례

- **실시간 대시보드**: 현재 사용자 활동, 페이지 조회, 검색어 등을 실시간으로 시각화
- **이상 탐지**: 갑작스러운 트래픽 급증, 오류 증가, 비정상적인 사용자 행동 탐지
- **보안 모니터링**: 연속된 로그인 실패, 비정상적인 접근 패턴 감지
- **실시간 성능 모니터링**: 응답 시간 지연, 오류율 증가 등을 즉시 감지하여 경고

### 7.4 통합 로그 분석 아키텍처

웹 서비스의 로그를 효과적으로 분석하기 위한 통합 아키텍처 구성 예시입니다.

```
웹 서비스 → Fluentd → Kafka → Spark Streaming → 실시간 대시보드
      ↓                  ↓                          ↓
    로그 파일        HDFS/S3            Elasticsearch/Grafana
                       ↓
                  Spark 배치 처리 → 데이터 웨어하우스 → BI 도구
```

#### 7.4.1 데이터 흐름

1. **로그 생성**: 웹 서비스에서 JSON 형식의 로그 생성
2. **수집 및 전달**: Fluentd가 로그를 수집하여 Kafka로 전달하고, 동시에 장기 보관을 위해 HDFS/S3에 저장
3. **실시간 처리**: Spark Streaming이 Kafka에서 로그를 소비하여 실시간 분석 수행
4. **배치 처리**: 일/주/월 단위로 HDFS/S3의 로그를 Spark으로 집계 분석
5. **데이터 저장**: 실시간 및 배치 분석 결과를 Elasticsearch, 데이터 웨어하우스 등에 저장
6. **시각화 및 분석**: Grafana, Tableau, Looker 등의 도구를 통해 데이터 시각화 및 분석

## 8. 로그 관리 모범 사례

1. **로그 보존 정책**: 로그 파일의 크기가 너무 커지지 않도록 로그 순환(rotation) 정책을 설정해야 합니다.

2. **개인정보 보호**: 사용자의 개인 식별 정보(PII)는 필요 시 암호화하거나 마스킹 처리해야 합니다.

3. **정기적인 백업**: 중요한 로그 데이터는 정기적으로 백업해야 합니다.

4. **로그 분석 자동화**: 중요한 지표는 자동화된 분석 파이프라인을 구축하여 모니터링해야 합니다.

## 9. 문제 해결 시 로그 활용

특정 문제가 발생했을 때 로그를 확인하는 방법은 다음과 같습니다:

1. **시스템 오류 확인**:
   ```bash
   grep ERROR 01_webservice/logs/app.log
   ```

2. **특정 사용자의 활동 조회**:
   ```bash
   grep "user_id\": 1" 01_webservice/logs/user_activity.log
   ```

3. **특정 상품 관련 활동 조회**:
   ```bash
   grep "entity_id\": 5" 01_webservice/logs/user_activity.log
   ```

4. **로그인 실패 기록 확인**:
   ```bash
   grep "login_failed" 01_webservice/logs/user_activity.log
   ```

## 10. 로그 기반 비즈니스 인텔리전스

로그 데이터는 기술적 모니터링 뿐만 아니라 비즈니스 인텔리전스로도 활용할 수 있습니다:

### 10.1 마케팅 최적화

- **최적의 마케팅 시간**: 사용자 활동이 가장 활발한 시간대 파악
- **인기 제품 및 카테고리**: 조회수가 높은 제품과 카테고리 식별
- **검색어 트렌드**: 인기 검색어 분석으로 사용자 관심사 파악
- **전환율 분석**: 페이지 방문에서 구매까지의 전환율 측정

### 10.2 제품 개선

- **사용자 경로 분석**: 사용자가 제품을 찾고 구매하는 과정 분석
- **기능 사용 추적**: 웹 서비스의 특정 기능 사용률 측정
- **이탈 지점 식별**: 사용자가 웹 서비스를 이탈하는 지점 식별
- **오류 발생 페이지**: 오류가 자주 발생하는 페이지 식별 및 개선

### 10.3 고객 행동 분석

- **고객 세그멘테이션**: 행동 패턴에 따른 고객 그룹화
- **개인화 추천**: 사용자 활동 기반 맞춤형 제품 추천
- **재방문 패턴**: 사용자의 재방문 주기 및 패턴 분석
- **충성도 측정**: 방문 빈도 및 활동량 기반 고객 충성도 평가

## 11. 요약

웹 서비스의 로그 시스템은 다음과 같은 핵심 기능을 제공합니다:

1. **JSON 형식**의 구조화된 로그로 분석이 용이함
2. **시스템 로그**와 **사용자 활동 로그**의 분리로 목적에 맞는 모니터링 가능
3. **다양한 이벤트 유형**을 제공하여 세밀한 사용자 행동 추적 가능
4. **데이터베이스 저장**을 통한 영구적인 데이터 보존 및 복잡한 쿼리 지원
5. **Fluentd, Spark, Spark Streaming** 등의 도구를 활용한 고급 로그 처리 가능

로그 데이터는 웹 서비스의 개선, 문제 해결, 사용자 경험 향상을 위한 귀중한 자원입니다. 적절히 활용하면 데이터 기반 의사 결정에 큰 도움이 됩니다. 

세 번째 인스턴스 구성 제안

```
첫 번째 인스턴스         두 번째 인스턴스         세 번째 인스턴스
(데이터 생성)          (데이터 저장/검색)       (데이터 처리/분석)
┌─────────────┐        ┌──────────────┐       ┌────────────────┐
│ 웹 서비스    │        │ OpenSearch   │       │ HDFS           │
│ + Fluentd   │───────▶│ + Dashboard  │──────▶│ + PySpark      │
└─────────────┘        └──────────────┘       │ + Zeppelin     │
                                              │ + Airflow      │
                                              └────────────────┘
                                                      │
                                                      ▼
                                              ┌────────────────┐
                                              │ Data Warehouse │
                                              │ → Data Mart    │
                                              └────────────────┘ 
```

이 접근 방식은 실무에서 사용하는 데이터 파이프라인 아키텍처와 유사하며, 학생들에게 데이터 엔지니어링의 전체 과정을 명확하게 보여줄 수 있습니다. 